{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version includes code copied from the website, especially when it comes to building the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
      " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
      " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
      " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
      " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n",
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt #allows to load data from a text file. Documentation here: https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html\n",
    "\n",
    "dataset = loadtxt('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv', delimiter=',') #delimiter is what separates the diferent values\n",
    "print(dataset)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output\n",
      "------\n",
      "x shape is: (768, 8) while x type is: <class 'numpy.ndarray'>\n",
      "y shape is: (768,) while y type is: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "print(f'Output')\n",
    "print(f'------')\n",
    "print(f'x shape is:', x.shape, 'while x type is:', type(x))\n",
    "print(f'y shape is:', y.shape, 'while y type is:', type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import talos\n",
    "#from talos.utils.metrics import f1score \n",
    "# for some reason, this does not work so I just quoted the entire path directly. Might be worth looking into why it didnt work\n",
    "#from talos.utils.callbacks import TrainingPlot\n",
    "# same with this\n",
    "\n",
    "def diabetes(x_train, y_train, x_val, y_val, params):\n",
    "    #This, here, is the model\n",
    "    #Parameters under iteration are: first neuron size, first activation, kernel initializer, first dropout, second neuron size, second activation, second dropout, last activation\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['first_neuron'], input_dim = x_train.shape[1], activation = params['first_activation'], kernel_initializer = params['kernel_initializer']))\n",
    "    model.add(Dropout(params['first_dropout']))\n",
    "    model.add(Dense(1, activation = params['last_activation']))\n",
    "\n",
    "    #At this point, we are compiling. The iterations are: loss, optimizer and the metrics in use are accuracy and f1score\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = params ['optimizer'], metrics = ['accuracy', talos.utils.metrics.f1score])\n",
    "\n",
    "    #Then we do the model fitting to the data and assign the result to 'out' variable. Iterations are: batch_size and epochs\n",
    "    # out = model.fit(x_train, y_train, validation_data = [x_val, y_val], batch_size = params['batch_size'], callbacks = [talos.callbacks.TrainingPlot(metrics = ['f1score'])], epochs = params['epochs'], verbose = 0)\n",
    "\n",
    "    #We remove the callback to plot f1score because we want to see the progress bar\n",
    "    out = model.fit(x_train, y_train, validation_data = [x_val, y_val], batch_size = params['batch_size'], epochs = 100, verbose = 0)\n",
    "\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import relu, elu, sigmoid, tanh #since these are only available in the library , we have to import the library itself for the model to interpret the dictionary accordingly\n",
    "#The model can interpret the functions on its own if its fed directly, but now we are not feeding it directly from its own internal dictionary, we are feeding it from the dictionary that we created hence need to import it to our dictionary first\n",
    "p = {\n",
    "    'lr':[0.5,5,10],\n",
    "    'hidden_layers': [0,1,2],\n",
    "    'epochs':[20,30,40],\n",
    "    'first_dropout':[0,0.2,0.25],\n",
    "    'first_neuron': [12, 24, 36],\n",
    "    'first_activation': ['relu', 'elu'],\n",
    "    'kernel_initializer': ['uniform', 'normal'],\n",
    "    'second_neuron': [12, 24, 36],\n",
    "    'second_activation': ['relu', 'elu'],\n",
    "    'second_dropout':[0,0.2,0.4],\n",
    "    'last_activation': ['sigmoid', 'tanh'],\n",
    "    'optimizer': ['Adam', 'Nadam'],\n",
    "    'losses': ['binary_crossentropy'],\n",
    "    'batch_size':[20,40],\n",
    "    'epochs':[50,100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/93312 [00:15<392:55:31, 15.16s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#this is the .scan method that I wrote about in the paper.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#the first two arguments make sense. params is the dictionary with the parameters\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#model is the name of the model that you just built. In our case, we encased our model into a function hence the name of the function. Otherwise, we would have model being model (since that is the name of our model)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#Experiment_name is used to create the experiment's log folder (where the data is stored)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m t \u001b[39m=\u001b[39m talos\u001b[39m.\u001b[39;49mScan(x \u001b[39m=\u001b[39;49m x, y \u001b[39m=\u001b[39;49m y, params \u001b[39m=\u001b[39;49m p, model \u001b[39m=\u001b[39;49m diabetes, experiment_name \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdiabetes\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m t\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39manalysis.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\talos\\scan\\Scan.py:205\u001b[0m, in \u001b[0;36mScan.__init__\u001b[1;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, multi_input, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights, save_models)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39m# start runtime\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mscan_run\u001b[39;00m \u001b[39mimport\u001b[39;00m scan_run\n\u001b[1;32m--> 205\u001b[0m scan_run(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\talos\\scan\\scan_run.py:26\u001b[0m, in \u001b[0;36mscan_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39m# otherwise proceed with next permutation\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mscan_round\u001b[39;00m \u001b[39mimport\u001b[39;00m scan_round\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m scan_round(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m     27\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[39m# close progress bar before finishing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\talos\\scan\\scan_round.py:19\u001b[0m, in \u001b[0;36mscan_round\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mingest_model\u001b[39;00m \u001b[39mimport\u001b[39;00m ingest_model\n\u001b[1;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_history, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mround_model \u001b[39m=\u001b[39m ingest_model(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mround_history\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_history\u001b[39m.\u001b[39mhistory)\n\u001b[0;32m     22\u001b[0m \u001b[39m# handle logging of results\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\talos\\model\\ingest_model.py:6\u001b[0m, in \u001b[0;36mingest_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mingest_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Ingests the model that is input by the user\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m    through Scan() model paramater.'''\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_train,\n\u001b[0;32m      7\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_train,\n\u001b[0;32m      8\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_val,\n\u001b[0;32m      9\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_val,\n\u001b[0;32m     10\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mround_params)\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mdiabetes\u001b[1;34m(x_train, y_train, x_val, y_val, params)\u001b[0m\n\u001b[0;32m     18\u001b[0m model\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer \u001b[39m=\u001b[39m params [\u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m], metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, talos\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mf1score])\n\u001b[0;32m     20\u001b[0m \u001b[39m#Then we do the model fitting to the data and assign the result to 'out' variable. Iterations are: batch_size and epochs\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# out = model.fit(x_train, y_train, validation_data = [x_val, y_val], batch_size = params['batch_size'], callbacks = [talos.callbacks.TrainingPlot(metrics = ['f1score'])], epochs = params['epochs'], verbose = 0)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[39m#We remove the callback to plot f1score because we want to see the progress bar\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, validation_data \u001b[39m=\u001b[39;49m [x_val, y_val], batch_size \u001b[39m=\u001b[39;49m params[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m], epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m out, model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1777\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1778\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1789\u001b[0m         pss_evaluation_shards\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1790\u001b[0m     )\n\u001b[1;32m-> 1791\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1792\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1793\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1794\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1795\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1796\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1797\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1798\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1799\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1800\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1801\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1802\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1803\u001b[0m )\n\u001b[0;32m   1804\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[0;32m   1805\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1806\u001b[0m }\n\u001b[0;32m   1807\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:2189\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_counter\u001b[39m.\u001b[39massign(\u001b[39m0\u001b[39m)\n\u001b[0;32m   2188\u001b[0m callbacks\u001b[39m.\u001b[39mon_test_begin()\n\u001b[1;32m-> 2189\u001b[0m \u001b[39mfor\u001b[39;00m (\n\u001b[0;32m   2190\u001b[0m     _,\n\u001b[0;32m   2191\u001b[0m     dataset_or_iterator,\n\u001b[0;32m   2192\u001b[0m ) \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   2194\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\data_adapter.py:1331\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1331\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1332\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1333\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:506\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    505\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:710\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    706\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    707\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    708\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 710\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    746\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[0;32m    747\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[0;32m    748\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 749\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3451\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3450\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3451\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3452\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3453\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3454\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this is the .scan method that I wrote about in the paper.\n",
    "#the first two arguments make sense. params is the dictionary with the parameters\n",
    "#model is the name of the model that you just built. In our case, we encased our model into a function hence the name of the function. Otherwise, we would have model being model (since that is the name of our model)\n",
    "#Experiment_name is used to create the experiment's log folder (where the data is stored)\n",
    "t = talos.Scan(x = x, y = y, params = p, model = diabetes, experiment_name = 'diabetes')\n",
    "t.data.to_csv('analysis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
