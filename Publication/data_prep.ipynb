{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE START BY CREATING A PARENT CLASS THAT WILL BE LOADING AND PREPARING ALL THE DATA FOR THE MODELS THAT WE ARE GOING TO BUILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class Data_prep:\n",
    "    def __init__(self):\n",
    "        self.df = None #Initialise the dataframe as none  (We could get rid of this as well. It's just because we already set it up)\n",
    "\n",
    "    def load_csv(self, filepath, weeks = 2):\n",
    "        '''Loads all the csv files present in the demand data folder, generates a timestamp with date and time (in 30 minute windows) and joins the csv files chronologically then filters for dates between 2015 and 2019\n",
    "\n",
    "        Input: path to folder. Number of weeks to be used for training. Unless user gives a different input, this is set to 2 but is automatically updated if user provides different input\n",
    "\n",
    "        Output: Updates the csv file created at the initialisation point. Also, a method variable called training duration'''\n",
    "\n",
    "        #Create a list of all csv files using the provided filepath\n",
    "        all_csv_files = glob.glob(filepath + '/*.csv')\n",
    "        \n",
    "        #Create empty demand list onto which we will read csv files as individual lists of files\n",
    "        demand_list = []\n",
    "\n",
    "        #loop to read through all the filenames that we have created using glob module - those that end with csv\n",
    "        for filename in all_csv_files:\n",
    "            #reading individual csv file in each cycle\n",
    "            df = pd.read_csv(filename, index_col=None, header = 0)\n",
    "            #extract that specific's file first settlement date as the start date - using iloc\n",
    "            date = pd.to_datetime(df['SETTLEMENT_DATE'].iloc[0])\n",
    "            #create an empty list onto which we will be appending all the generated timestamps\n",
    "            timestamps = []\n",
    "            #for loop to generate the timestamps of corresponding length to the dataframe and in timesteps of 30 minutes then append that data to the timestamps list\n",
    "            for i in range(0, len(df)*30, 30):\n",
    "                timestamps.append(date + timedelta(minutes=i))\n",
    "            #adding the new timestamps lists as an extra column for time identification in the new dataframe\n",
    "            df['date'] = timestamps\n",
    "            #appending the new dataframe (with each cycle) into the list of the dataframes\n",
    "            demand_list.append(df)\n",
    "\n",
    "        #Now concatenate all the lists into one dataframe\n",
    "        df_demand = pd.concat(demand_list, axis = 0, ignore_index=True)\n",
    "\n",
    "        #Filter for only pre-covid demand\n",
    "        df_demand = df_demand[df_demand['date'] < '2020-01-01 00:00:00']\n",
    "        df_demand = df_demand[df_demand['date'] > '2014-12-31 23:30:00']\n",
    "\n",
    "        #Reset the index to identify the split points better later and update df at the init point\n",
    "        self.df = df_demand.reset_index(drop = True)\n",
    "        \n",
    "        #Compute the training duration depending on the number of weeks provided by user\n",
    "        training_duration = weeks*7*48\n",
    "\n",
    "        #Print on terminal to show user the method output\n",
    "        print('DATA LOADING')\n",
    "        print(f'Input data runs from ', df_demand['date'].iloc[0], 'to', df_demand['date'].iloc[-1], 'and with', len(df_demand), 'datapoints')\n",
    "        print(f'Training duration is', training_duration, ' timesteps, an equivalent of', weeks, 'weeks')\n",
    "        \n",
    "        #return the training duration as a method variable\n",
    "        return training_duration\n",
    "\n",
    "    def train_val_test_split(self, training_duration, date_1 = '2016-01-01 00:00:00', date_2 = '2017-01-01 00:00:00', date_3 = '2018-01-01 00:00:00'):\n",
    "\n",
    "        '''Picks the loaded csv file and using the date column, identifies the split points the performs the splitting. Note that before the split, it retrieves the column to be split and converts it to a list of floats in GW (division by 1000) rather than MW. Note that to change the split points, the user has to update the dates manually in the code. Note that the dates for splitting have been preset but the user can provide appropriate dates in the provided format.\n",
    "\n",
    "        Input - training_duration, dates and the initialised/ updated dataframe df\n",
    "        Output - method-level updates train, val and test sets'''\n",
    "        if self.df is not None:\n",
    "            #RETRIEVE DEMAND AND TRAINING DURATION DATA, CONVERT IT TO A LIST OF FLOAT VALUES AFTER MOVING IT FROM MW TO GW\n",
    "            demand = list(map(float, (self.df['ND']/1000)))\n",
    "\n",
    "            #IDENTIFY THE SPLIT POINTS\n",
    "\n",
    "            #First years for training ('15). Adjust this when it comes to main training to 2015\n",
    "            split_point_one = self.df[self.df['date'] == date_1].index[0]\n",
    "\n",
    "            #'18 set aside as the valudation set and 2019 for the test set\n",
    "            split_point_two = self.df[self.df['date'] == date_2].index[0]\n",
    "\n",
    "            #'18 set aside as the valudation set and 2019 for the test set. Just for quick testing\n",
    "            split_point_three = self.df[self.df['date'] == date_3].index[0]\n",
    "\n",
    "            #PERFORMING THE SPLIT\n",
    "\n",
    "            #Then splitting the data with the training weeks being an overlap window\n",
    "            train, val, test = demand[0:split_point_one], demand[(split_point_one - training_duration):(split_point_two)], demand[(split_point_two - training_duration):split_point_three]\n",
    "\n",
    "            #Printing on terminal just to confirm to user that method is working\n",
    "            print('\\nSPLITTING')\n",
    "            print('Split results')\n",
    "            print('_____________') \n",
    "            print('Set Length Datapoint_1')\n",
    "            print(f'Train', len(train), train[0])\n",
    "            print(f'Val', len(val), val[0])\n",
    "            print(f'Test', len(test), test[0])\n",
    "\n",
    "        else:\n",
    "            print('Demand data not found, please provide path to folder first')\n",
    "        \n",
    "\n",
    "\n",
    "        return train, val, test\n",
    "    \n",
    "    def train_val_test_scaling(self, train, val, test):\n",
    "        '''Takes the previously updated train, validation and test sets, performs the scaling on them then updates them at the init point\n",
    "        Input - train, test and validation data which have been already instantiated\n",
    "        Output - an update to the instantiated train, test and validation data\n",
    "        Note that we have saved all the data at the init point to allow for the user to track the process by running each method hence ensure that everything is functional. Later on, we will update this such that '''\n",
    "\n",
    "        #PERFORMING NORMALISATION\n",
    "        #Create normalisation object\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # fit data. This identifies the max/ min demand and computes the mean and standard deviation to be used by all the other transformations\n",
    "        scaler.fit(np.array(train).reshape(-1, 1))\n",
    "\n",
    "        #Perform the transform but the data needs to be converted from list to array then reshaped\n",
    "        train_scaled = scaler.transform(np.array(train).reshape(-1, 1))\n",
    "        #Reshaping the train_scaled into a one dimensional array again\n",
    "        train_scaled = train_scaled.reshape(-1)\n",
    "\n",
    "        #Transforming the validation data\n",
    "        val_scaled = scaler.transform(np.array(val).reshape(-1,1))\n",
    "        #Reshaping the val_scaled into a one dimensional array again\n",
    "        val_scaled = val_scaled.reshape(-1)\n",
    "\n",
    "        #Fitting the test data on its own\n",
    "        scaler.fit(np.array(test).reshape(-1, 1))\n",
    "\n",
    "        #Transforming the test data on its own\n",
    "        test_scaled = scaler.transform(np.array(test).reshape(-1, 1))\n",
    "        #Reshaping the test_scaled into a one dimensional array again\n",
    "        test_scaled = test_scaled.reshape(-1)\n",
    "\n",
    "        #Print just to confirm to user that the values have changed hence MinMax transformation effective\n",
    "        print('\\nSCALING')\n",
    "        print('Scaled results')\n",
    "        print('_____________') \n",
    "        print('Set Length Datapoint_1')\n",
    "        print(f'Train', len(train_scaled), train_scaled[0])\n",
    "        print(f'Val', len(val_scaled), val_scaled[0])\n",
    "        print(f'Test', len(test_scaled), test_scaled[0])\n",
    "        \n",
    "        return train_scaled, val_scaled, test_scaled\n",
    "\n",
    "    def reshaping(self, data, n_features):\n",
    "        '''Takes the initialised and updated (following the scaling) train, val and test lists and reshapes them for the time series prediction model\n",
    "        The reshaping is implemeted through the split_sequence function which returns results in the shape of batches, features, batch_size. The method then reshapes this into batches, batch_size, features which is appropriate for time forecasting models\n",
    "        Input - previously initialised and updated demand data (lists), and the number of input features in the data as an integer (in this case, 1)\n",
    "        Output - the reshaped arrays in order of X_train, y_train, X_val, y_val, X_test, y_test. However, these are saved at the init method hence easily accessible'''\n",
    "\n",
    "        #SETTING UP THE FUNCTION TO PERFORM THE RESHAPING\n",
    "        def split_sequence(sequence, n_steps):\n",
    "            X, y = list(), list()\n",
    "            #loop through the list and update the days so that instead of sliding the window across one point, its across 48 points (one day)\n",
    "            for i in range(0, len(sequence), 48):\n",
    "                # find the end of this pattern\n",
    "                end_ix = i + n_steps\n",
    "                # check if we are beyond the sequence\n",
    "                if end_ix > len(sequence)-48:\n",
    "                    break\n",
    "                # gather input and output parts of the pattern\n",
    "                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix: end_ix+48]\n",
    "                X.append(seq_x)\n",
    "                y.append(seq_y)\n",
    "            return np.array(X), np.array(y)\n",
    "        #Calling the split_sequence function to split training into x_train and y_train\n",
    "        X_train, y_train = split_sequence(data[0], data[1])\n",
    "\n",
    "        #reshaping the x_train so that the features is the last value\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "\n",
    "        #Calling the split sequence function to split validation into x_val and y_val\n",
    "        X_val, y_val = split_sequence(data[2], data[1])\n",
    "        #reshaping the x_train so that the features is the last value\n",
    "        X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], n_features))\n",
    "\n",
    "        #Calling the split sequence function to split validation into x_val and y_val\n",
    "        X_test, y_test = split_sequence(data[3], data[1])\n",
    "\n",
    "        #reshaping the x_test so that the features is the last value\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "        print('\\n RESHAPING')\n",
    "        print('Reshaping results')\n",
    "        print('_________________')\n",
    "        print(f'X_train', X_train.shape)\n",
    "        print(f'y_train', y_train.shape)\n",
    "        print(f'X_val', X_val.shape)\n",
    "        print(f'y_val', y_val.shape)\n",
    "        print(f'X_test', X_test.shape)\n",
    "        print(f'y_test', y_test.shape)\n",
    "\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \n",
    "    def prepare_data(self, filepath, weeks=2, date_1 = '2016-01-01 00:00:00', date_2 = '2017-01-01 00:00:00', date_3 = '2018-01-01 00:00:00', n_features = 1):\n",
    "\n",
    "        #Loading csv - provide filepath and training weeks (optional)\n",
    "        training_duration = self.load_csv(filepath, weeks)\n",
    "\n",
    "        #Splitting - training duration provided for by previous method.\n",
    "        #Provide dates along which to perform the split (optional) - enter as string\n",
    "        train, val, test = self.train_val_test_split(training_duration, date_1, date_2, date_3)\n",
    "\n",
    "        #Scaling. All input data provided for by class methods\n",
    "        train_scaled, val_scaled, test_scaled = self.train_val_test_scaling(train, val, test)\n",
    "        \n",
    "        #Reshaping. Most input data provided for by previous class methods. Can update n_features depending on input features but by default, set to 1.\n",
    "        prepared_data = [train_scaled, training_duration, val_scaled, test_scaled]\n",
    "        return self.reshaping(prepared_data, n_features)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#create class object\n",
    "data_prep_inst = Data_prep()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS SECTION SHOWS HOW TO RUN THE METHODS ONE BY ONE - NOTE THAT THIS MUST BE DONE IN THE CORRECT ORDER AS METHODS DEPEND ON THE OUTPUT FROM OTHER METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the load_csv method to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADING\n",
      "Input data runs from  2015-01-01 00:00:00 to 2019-12-31 23:30:00 and with 87648 datapoints\n",
      "Training duration is 672  timesteps, an equivalent of 2 weeks\n"
     ]
    }
   ],
   "source": [
    "#We create a variable to hold onto the training_duration that the method is going to output\n",
    "training_duration_output = data_prep_inst.load_csv('Demand_data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPLITTING\n",
      "Split results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 28.726\n",
      "Val 18240 26.958\n",
      "Test 18192 28.007\n"
     ]
    }
   ],
   "source": [
    "#The output of the previous method is fed into the present method since it was saved as a variable. Then we save what this method produces as variables too\n",
    "train, val, test = data_prep_inst.train_val_test_split(training_duration_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SCALING\n",
      "Scaled results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 0.31256682174126627\n",
      "Val 18240 0.26147889155373194\n",
      "Test 18192 0.34324056561289745\n"
     ]
    }
   ],
   "source": [
    "#Produces three outputs(scaled) hence we are holding onto those. But at the same time, dependent on the three outputs from the previous method hence feed those into it.\n",
    "train_scaled, val_scaled, test_scaled = data_prep_inst.train_val_test_scaling(train, val, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RESHAPING\n",
      "Reshaping results\n",
      "_________________\n",
      "X_train (351, 672, 1)\n",
      "y_train (351, 48)\n",
      "X_val (366, 672, 1)\n",
      "y_val (366, 48)\n",
      "X_test (365, 672, 1)\n",
      "y_test (365, 48)\n"
     ]
    }
   ],
   "source": [
    "#First, we need to compile all the data into a list then we can feed that list into the method\n",
    "listed_data = [train_scaled, training_duration_output, val_scaled, test_scaled]\n",
    "\n",
    "#Then we call the method and save its output\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = data_prep_inst.reshaping(listed_data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined method\n",
    "Input data is as follows:\n",
    "1. path_to_folder (mandatory and as a string)\n",
    "2. The number of weeks for training (as integer but optional. Default is 2 weeks)\n",
    "3. Date up to which the training split should occur (as a string. Example: '2016-01-01 00:00:00'. By default, set to '2016-01-01 00:00:00')\n",
    "4. Date up to which the validation split should occur (as string. Example: '2016-01-01 00:00:00'. By default set to '2017-01-01 00:00:00'). Note that once set, the split will happen from the date provided in step 3 to step 4 but with an overlapping window back into training set by the 2 weeks of training to ensure match in forecast dates for both validation and test sets\n",
    "5. Date up to which the test split should occur (as string. Example: '2017-01-01 00:00:00'. By default, set to '2018-01-01 00:00:00'). To use the entire dataset, update this to the last date on your dataframe as printed when loaded.\n",
    "6. The number of features in your data. (as integer. Mandatory if dealing with multivariate models since by default, is set to 1 i.e single input feature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADING\n",
      "Input data runs from  2015-01-01 00:00:00 to 2019-12-31 23:30:00 and with 87648 datapoints\n",
      "Training duration is 672  timesteps, an equivalent of 2 weeks\n",
      "\n",
      "SPLITTING\n",
      "Split results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 28.726\n",
      "Val 18240 26.958\n",
      "Test 18192 28.007\n",
      "\n",
      "SCALING\n",
      "Scaled results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 0.31256682174126627\n",
      "Val 18240 0.26147889155373194\n",
      "Test 18192 0.34324056561289745\n",
      "\n",
      " RESHAPING\n",
      "Reshaping results\n",
      "_________________\n",
      "X_train (351, 672, 1)\n",
      "y_train (351, 48)\n",
      "X_val (366, 672, 1)\n",
      "y_val (366, 48)\n",
      "X_test (365, 672, 1)\n",
      "y_test (365, 48)\n"
     ]
    }
   ],
   "source": [
    "data_prep_inst.prepare_data('Demand_data',2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
