{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution layer shape: (None, 670, 64)\n",
      "bilstm layer shape: (None, 672, 200)\n",
      "padding layer shape: (None, 672, 64)\n",
      "merged layer shape: 672\n",
      "reshaped layer shape: (None, 264, 672)\n",
      "max_pool_1D layer shape: (None, 672)\n",
      "output_layer layer shape: (None, 48)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from IPython.display import clear_output\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#Create the input node. We omit batch size when using functional API\n",
    "inputs = keras.Input(shape = (672, 1))\n",
    "\n",
    "\n",
    "#Then we implement the convolution layer\n",
    "con_1D = keras.layers.Conv1D(filters=64, kernel_size=3)(inputs)\n",
    "print(f'convolution layer shape:', con_1D.shape)\n",
    "\n",
    "#Then we construct the parallel bilstm layer\n",
    "biLSTM = Bidirectional(LSTM(100,return_sequences=True))(inputs)\n",
    "biLSTM= Dropout(0.2)(biLSTM)\n",
    "print(f'bilstm layer shape:', biLSTM.shape)\n",
    "\n",
    "# ADDING PADDING SEQUENCE TO MAKE THEM COMPATIBLE BEFORE CONCATENATION\n",
    "\n",
    "#first we compute the length difference between the two outputs\n",
    "pad_difference = (biLSTM.shape[1] - con_1D.shape[1])\n",
    "\n",
    "#then we implement a for loop to ensure padding to equal lenth even if odd lenth difference\n",
    "#// double division is to give us an integer rather than a float output\n",
    "if (pad_difference) % 2 == 0:\n",
    "    pad_1, pad_2 = pad_difference // 2, pad_difference // 2\n",
    "else:\n",
    "    pad_1, pad_2 = pad_difference // 2, (pad_difference // 2) + 1\n",
    "\n",
    "#Now applying the padding to the convolution layer\n",
    "padding_layer = keras.layers.ZeroPadding1D(padding=(pad_1, pad_2))(con_1D)\n",
    "print(f'padding layer shape:', padding_layer.shape)\n",
    "\n",
    "#Then we merge\n",
    "merged = keras.layers.Concatenate()([padding_layer,biLSTM])\n",
    "print(f'merged layer shape:', merged.shape[1])\n",
    "\n",
    "#Then we reshape such that steps is the last item on that list\n",
    "reshaped = Reshape((264, 672)) (merged)\n",
    "print(f'reshaped layer shape:', reshaped.shape)\n",
    "#Then we add a bilstm to condense the 264 variants into 1. Note, unlike in first bilstm, return-sequence set to false by not being declared\n",
    "max_pool_1D = GlobalMaxPooling1D()(reshaped)\n",
    "print(f'max_pool_1D layer shape:', max_pool_1D.shape)\n",
    "\n",
    "#Then the final output layer\n",
    "output_layer = Dense(48)(max_pool_1D)\n",
    "print(f'output_layer layer shape:', output_layer.shape)\n",
    "\n",
    "# #Then pull everything together to build the final model\n",
    "# model = keras.models.Model(inputs=inputs, outputs=output_layer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
