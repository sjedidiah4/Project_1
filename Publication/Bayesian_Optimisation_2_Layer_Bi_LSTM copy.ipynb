{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IyCiDR1BOXE"
      },
      "source": [
        "LOADING THE ELECTRICITY DEMAND DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-LjZ3R9BO-R",
        "outputId": "d76f0e0d-fded-4a20-93f4-3916819204cc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from datetime import timedelta\n",
        "\n",
        "#path to the drive folder\n",
        "path = \"Demand_data\"\n",
        "#instruction for all files in the folder that end with csv to be appended onto a list using the glob module\n",
        "all_csv_files = glob.glob(path + \"/*.csv\")\n",
        "#creating empty demand list onto which we will add the read csv files as individual lists of files, imagine like in a drawer\n",
        "demand_list = []\n",
        "\n",
        "#loop to read through all the filenames that we have created using glob module - those that end with csv\n",
        "for filename in all_csv_files:\n",
        "    #reading individual csv file in each cycle\n",
        "    df = pd.read_csv(filename, index_col=None, header = 0)\n",
        "    #extract that specific's file first settlement date as the start date - using iloc\n",
        "    date = pd.to_datetime(df['SETTLEMENT_DATE'].iloc[0])\n",
        "    #create an empty list onto which we will be appending all the generated timestamps\n",
        "    timestamps = []\n",
        "    #for loop to generate the timestamps of corresponding length to the dataframe and in timesteps of 30 minutes then append that data to the timestamps list\n",
        "    for i in range(0, len(df)*30, 30):\n",
        "      timestamps.append(date + timedelta(minutes=i))\n",
        "    #adding the new timestamps lists as an extra column for time identification in the new dataframe\n",
        "    df['date'] = timestamps\n",
        "    #appending the new dataframe (with each cycle) into the list of the dataframes\n",
        "    demand_list.append(df)\n",
        "\n",
        "#Now concatenate all the lists into one dataframe\n",
        "df_demand = pd.concat(demand_list, axis = 0, ignore_index=True)\n",
        "\n",
        "#Filter for only pre-covid demand\n",
        "df_demand = df_demand[df_demand['date'] < '2020-01-01 00:00:00']\n",
        "df_demand = df_demand[df_demand['date'] > '2014-12-31 23:30:00']\n",
        "\n",
        "#Reset the index to identify the split points better later\n",
        "df_demand = df_demand.reset_index(drop = True)\n",
        "\n",
        "#Terminal prints to confirm filtered dataframe start and end dates\n",
        "print(f'First day:', df_demand['date'].iloc[0])\n",
        "print(f'Last day:', df_demand['date'].iloc[-1])\n",
        "print(f'dataframe length:', len(df_demand))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMhyLtLBUqL"
      },
      "source": [
        "EXTRATING AND ADJUSTING THE DEMAND DATA FOR MODEL PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuVJ4wDcBW1a",
        "outputId": "95e68d07-d879-4c87-e1c5-c0cbc93f19f9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "#extract demand data from dataframe and convert it into a Gigawatt list (157776 elements)\n",
        "demand = list(map(float, (df_demand['ND']/1000)))\n",
        "\n",
        "#Terminal print to confirm list same length as dataframe and data in GW instead of MW\n",
        "print(f'demand data length:', len(demand))\n",
        "print(f'first ten demand elements in GW before normalisation:', demand[:10])\n",
        "print(f'last ten demand elements in GW before normalisation:', demand[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na3Gxbj2yfp6"
      },
      "source": [
        "Subsection: Identify the split point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJRLyVyGykIi",
        "outputId": "e9368b30-0eff-47d1-aedf-6580050b5015"
      },
      "outputs": [],
      "source": [
        "#First three years for training ('15, '16, 17)\n",
        "split_point_one = df_demand[df_demand['date'] == '2016-01-01 00:00:00'].index[0]\n",
        "\n",
        "#'18 set aside as the valudation set and 2019 for the test set\n",
        "split_point_two = df_demand[df_demand['date'] == '2017-01-01 00:00:00'].index[0]\n",
        "\n",
        "#'18 set aside as the valudation set and 2019 for the test set. Just for quick testing\n",
        "split_point_three = df_demand[df_demand['date'] == '2018-01-01 00:00:00'].index[0]\n",
        "\n",
        "#Terminal print to confirm split points\n",
        "print(f'The split point is at index', split_point_one, 'while the second one is:', split_point_two)\n",
        "print(f'the third split point is', split_point_three)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2aGLsI_yp9V"
      },
      "source": [
        "Subsection: perform the split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2_jvtd0ytvs",
        "outputId": "60e45be0-6cbc-422d-ace2-2a8a9bd0028b"
      },
      "outputs": [],
      "source": [
        "#apply that split point on the list of demand data careful to include the slight overlap in test set to ensure no skip in predictions\n",
        "#set the training window - 2 weeks (2 weeks *7 days *48 settlement period) input to map out to one day output (48 settlement periods)\n",
        "n_steps = 2*7*48\n",
        "train, val, test = demand[0:split_point_one], demand[(split_point_one - n_steps):(split_point_two)], demand[(split_point_two - n_steps):split_point_three]\n",
        "print(f'Training set length is', len(train), 'validation set is:', len(val),'while test set length is', len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyqM8qB8y8Fq"
      },
      "source": [
        "Subsection: normalising the data using the minmax scaler algorithm. Note that I tried running without normalisation and the model training quickly runs into a nan loss within the first few training batches. Also, the other program file explores the use of the standard scaler as an alternative. Normalisation temporarily excluded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqjQLnhwy_Kc",
        "outputId": "ec4f1cff-2206-4983-ba26-b244f3989959"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# define min max scaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit data. This identifies the max/ min demand and computes the mean and standard deviation to be used by all the other transformations\n",
        "print(scaler.fit(np.array(train).reshape(-1, 1)))\n",
        "\n",
        "#Terminal print to identify what the maximum demand in the training set was\n",
        "print(scaler.data_max_)\n",
        "\n",
        "#Now that we have identified, we perform the tranform but the data needs to be converted from list to array then reshaped\n",
        "train = scaler.transform(np.array(train).reshape(-1, 1))\n",
        "print(train.shape)\n",
        "\n",
        "#Since we reshaped the data to a single stretching column, we now reconvert that to a horizontal array\n",
        "train = train.reshape(-1)\n",
        "print(f'First 5 elements of transformed training data:', train[0:5])\n",
        "\n",
        "#Transforming the validation data\n",
        "val = scaler.transform(np.array(val).reshape(-1,1))\n",
        "print(val.shape)\n",
        "\n",
        "#Reconverting validation data to a horizontal array again\n",
        "val = val.reshape(-1)\n",
        "print(f'First 5 elements of transformed validation data:', val[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGVxPoZazAR3"
      },
      "source": [
        "Now creating the function to reshape the data before feeding it into the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p7Cxvn5-B8rp"
      },
      "outputs": [],
      "source": [
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        " #loop through the list and update the days so that instead of sliding the window across one point, its across 48 points (one day)\n",
        "\tfor i in range(0, len(sequence), 48):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-48:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix: end_ix+48]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz0Wo3ClKrYP"
      },
      "source": [
        "Subsection: reshape training data for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0UbpukuIiD2",
        "outputId": "1aa95bb1-f434-435f-fe63-1910790dd40b"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = split_sequence(train, n_steps)\n",
        "\n",
        "#Useful to understanding the new shaping\n",
        "print('X_train data')\n",
        "print('------------')\n",
        "print(X_train.shape)\n",
        "print(len(X_train[0]))\n",
        "print(X_train[0][0:6])\n",
        "\n",
        "print('y_train data')\n",
        "print('------------')\n",
        "print(y_train.shape)\n",
        "print(len(y_train[0]))\n",
        "print(y_train[0][0:6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi9tJk55wceB"
      },
      "source": [
        "Subsection: reshape the validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW0PTna2wfLn",
        "outputId": "56528298-b081-49ce-acc0-44aa5926456b"
      },
      "outputs": [],
      "source": [
        "X_val, y_val = split_sequence(val, n_steps)\n",
        "\n",
        "#Useful to understanding the new shaping\n",
        "print('X_val data')\n",
        "print('------------')\n",
        "print(X_val.shape)\n",
        "print(len(X_val[0]))\n",
        "print(X_val[0][0:6])\n",
        "\n",
        "print('y_val data')\n",
        "print('------------')\n",
        "print(y_val.shape)\n",
        "print(len(y_val[0]))\n",
        "print(y_val[0][0:6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwBdafaMCF6M"
      },
      "source": [
        "Subsection: reshaping the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVvH3-bwCGiy",
        "outputId": "28cdd90a-52e3-47d8-8c51-a37145ad7a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the new x_train shape for the model is (351, 672, 1)\n"
          ]
        }
      ],
      "source": [
        "#univariate hence n_features = 1\n",
        "n_features = 1\n",
        "#This reshapes the data such that if you imagine you are staring at a screen:\n",
        "#n_features is the number of vertical lines you can draw on that screen\n",
        "#X_train.shape[1], in this case, 672,is the number of lines you could draw from left to right on the same screen\n",
        "#X_train.shape[0], in this case 1082 (the number of batched samples) is the number of lines you could draw into or out of the screen\n",
        "#If you imagine the screen that you are facing being pushed to your right, almost like into a slot, then what we have done is reshape the data\n",
        "#such that the past 672 demand points are written vertically and being pushed as one long vertical line into the slot and will be mapped to the 48 we have on the other end\n",
        "#This mapping will happen 139537 times in cycle (epoch) during which the model will try to identify any patterns.\n",
        "#The learning is then repeated across several epochs\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
        "print(f'the new x_train shape for the model is', X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzlKQIdiySNQ"
      },
      "source": [
        "Subsection: building the autoregressive model. Note that I had to lower the learning rate by 10 factor 3 (default is 0.0001) to avoid the training running into infinite then nan loss values (which before, was happening by the time we got to the 30th of 4361 batches). I also added the clipvalue to try prevent the same but might be worthwhile to see how the model would perform if we were to remove it.\n",
        "\n",
        "Updated to include the live plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJRLeGgxl4xE",
        "outputId": "169a458b-be36-441e-c355-5a2fd1f0d89a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from IPython.display import clear_output\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "#CREATING DATAFRAME TO SAVE RELEVANT RESULTS\n",
        "results_df = pd.DataFrame(columns=[\"Iteration\", \"Start Time\", \"End Time\", \"unit1\", \"unit2\", \"drop1\", \"drop2\", \"learn_rate\", \"step1\", \"step2\", \"step3\", \"rate1\", \"rate2\", \"rate3\", \"Train Loss\", \"Val Loss\", \"Train MSE\", \"Val MSE\"])\n",
        "\n",
        "#SET UP TRAINING PARAMETERS\n",
        "\n",
        "# Function to calculate root mean squared error (RMSE)\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
        "\n",
        "#set batch_size\n",
        "batch_size = int(X_train.shape[0]/8)\n",
        "print(f'Batch size:', batch_size)\n",
        "\n",
        "def optimize_lstm(unit1, drop1,unit2,drop2,learn_rate,step1,rate1,step2,rate2, step3, rate3):\n",
        "\n",
        "    #These are some of the hyperparameters that we will be adjusting using bayesian optimisation, though some of them are, actually, parameters in that they stay constant throughout the exploration process. Example, the intial learning rate\n",
        "    unit1 = int(unit1)\n",
        "    unit2 = int(unit2)\n",
        "    step1 = int(step1)\n",
        "    step2 = int(step2)\n",
        "    step3 = int(step3)\n",
        "    initial_learning_rate = learn_rate\n",
        "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay([step1, step2, step3], [learn_rate, rate1,rate2,rate3])\n",
        "    #What this is saying is this, take, for example, the first iteration being step1   (0,500), learn_rate1(0.0005) - which is now the initial learning rate and rate1(0.0001, 0.001). This is saying that for the first 0th iteration, train with initial learning rate - 0.0005, for the next 500 iterations, train with first element of rate1 - 0.0001 and for the remainder, train with the new rate - 0.001. Include link (ChatGPT Optimising hyperparameter note folder)\n",
        "\n",
        "    #Here we define the start time\n",
        "    start_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
        "\n",
        "\n",
        "    #This is where we build the actual model. Note unit1 is first_layer_neuron_size, drop_1 is dropout_rate for first dropout layer, unit2 is the number of neurons in the second bidirectional lstm layer then drop2 is the dropout rate for the second dropout layer. So we are adjusting layer1, rate1, layer2,rate2\n",
        "    model = Sequential([\n",
        "        tf.keras.layers.Bidirectional(LSTM(unit1, return_sequences = True, input_shape = (n_steps, n_features))),\n",
        "        tf.keras.layers.Dropout(drop1),\n",
        "        tf.keras.layers.Bidirectional(LSTM(unit2, return_sequences = False)),\n",
        "        tf.keras.layers.Dropout(drop2),\n",
        "        tf.keras.layers.Dense(48)\n",
        "    ])\n",
        "\n",
        "    #Now these are some of the training parameters to reduce on the training time. We are telling the model that for each training period, stop once you get to a point where even after 100 consecutive trainings, there is no change in the validation loss\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 100)\n",
        "\n",
        "    #This is now where we bring everything together into the model. First, we built the model architecture, then we defined a few other things to optimise the training such as early stopping then now we bring everything together with compile saying:\n",
        "    #when training the model, use Adam optimiser to find the best weights, use the following learning rate schedule\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule), loss = root_mean_squared_error, metrics = ['mse'])\n",
        "    model.build(input_shape = (None, n_steps, n_features))\n",
        "\n",
        "    history = model.fit(x = X_train,\n",
        "                        y = y_train,\n",
        "                        verbose = 2, #just want to see the epoch and the training/ validation losses. Don't care about time logs yet\n",
        "                        batch_size = batch_size,\n",
        "                        epochs = 5000,\n",
        "                        validation_data = [X_val, y_val],\n",
        "                        callbacks = [early_stop]\n",
        "                        )\n",
        "\n",
        "    #Obtaining all the results and saving them to a dictionary\n",
        "    iteration_results = {\n",
        "        \"Iteration\": len(results_df) + 1,\n",
        "        \"Start Time\": start_time,\n",
        "        \"End Time\": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),\n",
        "        \"unit1\": unit1,\n",
        "        \"unit2\": unit2,\n",
        "        \"drop1\": drop1,\n",
        "        \"drop2\": drop2,\n",
        "        \"learn_rate\": learn_rate,\n",
        "        \"step1\": step1,\n",
        "        \"step2\": step2,\n",
        "        \"step3\": step3,\n",
        "        \"rate1\": rate1,\n",
        "        \"rate2\": rate2,\n",
        "        \"rate3\": rate3,\n",
        "        \"Train Loss\": history.history[\"loss\"][-1],\n",
        "        \"Val Loss\": history.history[\"val_loss\"][-1],\n",
        "        \"Train MSE\": history.history[\"mse\"][-1],\n",
        "        \"Val MSE\": history.history[\"val_mse\"][-1]\n",
        "    }\n",
        "\n",
        "    # Append iteration results to results_df\n",
        "    results_df.loc[len(results_df)] = iteration_results\n",
        "\n",
        "    # Save results to CSV after each iteration\n",
        "    results_df.to_csv('Bayesian_BiLSTM_analysis.csv', index=False)\n",
        "\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "\n",
        "    return -best_val_loss\n",
        "\n",
        "\n",
        "#Dictionary to store the boundaries probably. Note that the way the Bayesian optimiser works is not discrete like with the talos gridsearch option. This is almost continuous\n",
        "\n",
        "###THIS IS THE SECTION THAT YOU NEED TO ADJUST DEPENDING ON YOUR PARAMETERS\n",
        "p_bounds = {\n",
        "    'unit1':(20,128), #first layer neurons - from 1 to 100\n",
        "    'unit2': (20,256),#second layer neurons - from 1 to 100\n",
        "    'drop1': (0.05,0.5), #first dropout rate - from 0.05 to 0.5\n",
        "    'drop2': (0.05,0.5), #second dropout rate\n",
        "    'learn_rate': (0.0005, 0.005), #there are two initial learning rates. First, start with 0.0005 and follow the schedule. Then restart with 0.005 and follow the schedule\n",
        "    'step1': (10, 500),\n",
        "    'step2': (1000, 2000),\n",
        "    'step3': (2000, 5000),\n",
        "    'rate1': (0.0001, 0.001),\n",
        "    'rate2': (0.00005, 0.0005),\n",
        "    'rate3': (0.000005, 0.00005)\n",
        "}\n",
        "\n",
        "#create an object for the optimiser\n",
        "optimizer = BayesianOptimization(f = optimize_lstm, pbounds = p_bounds, random_state = None, verbose = 2)\n",
        "\n",
        "#Then apply the .maximize method\n",
        "#This will generate 15 different sets of hyperparameters in the beginning (init_points)\n",
        "#Later, it will select a new set of hyperparameters (16th) using the surrogate model and keep doing this another 14 more times (n_iter = 15). In total, this model will run 30 iterations and the results will be saved into the dataframe that I created up there.\n",
        "optimizer.maximize(init_points = 15, n_iter = 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "THE END"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
