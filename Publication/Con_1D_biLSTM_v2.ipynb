{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from IPython.display import clear_output\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#We are inheriting all the properties available in that class (Data_prep) by passing it as an argument to the predictor class.\n",
    "from data_prep import Data_prep as prep\n",
    "\n",
    "class Con_1D_BiLSTM_v2(prep):\n",
    "    def __init__(self):\n",
    "        #Even though we inherited the properties, we had our parent class initialise certain attributes through the constructor (__init__). With inheritance, we need to explicitly invoke this initialisation through the use of super() method otherwise it it wil be bypassed\n",
    "        super().__init__()\n",
    "        self.results_df = pd.DataFrame(columns=[\"Iteration\", \"Start Time\", \"End Time\", \"con_1D_filters\", \"con_1D_kernel_size\", \"biLSTM_units\", \"biLSTM_dropout_rate\", 'biLSTM2_units',\"learn_rate\", \"step1\", \"step2\", \"step3\", \"rate1\", \"rate2\", \"rate3\", \"Train Loss\", \"Val Loss\", \"Train MSE\", \"Val MSE\"])\n",
    "\n",
    "    def prepare_data(self, filepath, weeks=2, date_1 = '2016-01-01 00:00:00', date_2 = '2017-01-01 00:00:00', date_3 = '2018-01-01 00:00:00', n_features = 1):\n",
    "\n",
    "        '''\n",
    "        Note that this is the method that inherits methods from the parent class and runs all the data preparation operations in one method call.\n",
    "\n",
    "        The method takes the path to the folder containing the input data, builds a dataframe, performs the splitting, scaling and eventual reshaping for the forecasting model. For more information, right click on the method in question and go to where it was declared.\n",
    "        \n",
    "        Inputs:\n",
    "        path_to_folder (mandatory and as a string)\n",
    "        \n",
    "        The number of weeks for training (as integer but optional. Default is 2 weeks)\n",
    "        \n",
    "        Date up to which the training split should occur (as a string. Example: '2016-01-01 00:00:00'. By default, set to '2016-01-01 00:00:00')\n",
    "         \n",
    "        Date up to which the validation split should occur (as string.Example: '2016-01-01 00:00:00'. By default set to '2017-01-01 00:00:00'). Note that once set, the split will happen from the date provided in step 3 to step 4 but with an overlapping window back into training set by the 2 weeks of training to ensure match in forecast dates for both validation and test sets\n",
    "        Date up to which the test split should occur (as string. Example: '2017-01-01 00:00:00'. By default, set to '2018-01-01 00:00:00'). To use the entire dataset, update this to the last date on your dataframe as printed when loaded.\n",
    "        \n",
    "        The number of features in your data. (as integer. Mandatory if dealing with multivariate models/ multi-feature datasets since by default, is set to 1 i.e single input feature.)\n",
    "        \n",
    "        Outputs:\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test\n",
    "        Note: Also prints on the terminal all the steps taken'''\n",
    "\n",
    "        #Loading csv - provide filepath and training weeks (optional)\n",
    "        training_duration = self.load_csv(filepath, weeks)\n",
    "        \n",
    "        #Splitting - training duration provided for by previous method.\n",
    "        # #Provide dates along which to perform the split (optional) - enter as string\n",
    "        train, val, test = self.train_val_test_split(training_duration, date_1, date_2, date_3)\n",
    "\n",
    "        #Scaling. All input data provided for by class methods\n",
    "        train_scaled, val_scaled, test_scaled = self.train_val_test_scaling(train, val, test)\n",
    "        \n",
    "        #Reshaping. Most input data provided for by previous class methods. Can update n_features depending on input features but by default, set to 1. \n",
    "        prepared_data = [train_scaled, training_duration, val_scaled, test_scaled]\n",
    "        \n",
    "        return self.reshaping(prepared_data, n_features)\n",
    "\n",
    "    def conv1D_bilstm_v2(self, con_1D_filters, con_1D_kernel_size, biLSTM_units, biLSTM_dropout_rate, printing = True, save_image=True, image_filename=\"model_architecture.png\"):\n",
    "\n",
    "        '''This method's focus is to build the different neural layers and aggregate them into a single model. The method automatically prints out a model summary to help make sense of the model that has just been built\n",
    "\n",
    "\n",
    "        Input - the model parameters that the user wants to make adjustable for bayesian optimisation process. So far, these are set to: con_1D_filters, con_1D_kernel_size, biLSTM_units, biLSTM_dropout_rate, biLSTM2_units\n",
    "\n",
    "        \n",
    "        Output - model summary with the different layers, their output shapes and the overall learning parameters. While this is printed on the terminal for the user to see, the model itself is held by the method to be used in the optimisation process.'''\n",
    "\n",
    "        #Create the input node. We omit batch size when using functional API\n",
    "        inputs = keras.Input(shape = (X_train.shape[1], X_train.shape[2]))\n",
    "        \n",
    "\n",
    "        #Then we implement the convolution layer\n",
    "        con_1D = keras.layers.Conv1D(filters=con_1D_filters, kernel_size=con_1D_kernel_size)(inputs)\n",
    "\n",
    "        #Then we construct the parallel bilstm layer\n",
    "        biLSTM = Bidirectional(LSTM(biLSTM_units,return_sequences=True))(inputs)\n",
    "        biLSTM= Dropout(biLSTM_dropout_rate)(biLSTM)\n",
    "\n",
    "        # ADDING PADDING SEQUENCE TO MAKE THEM COMPATIBLE BEFORE CONCATENATION\n",
    "\n",
    "        #first we compute the length difference between the two outputs\n",
    "        pad_difference = (biLSTM.shape[1] - con_1D.shape[1])\n",
    "\n",
    "        #then we implement a for loop to ensure padding to equal lenth even if odd lenth difference\n",
    "        #// double division is to give us an integer rather than a float output\n",
    "        if (pad_difference) % 2 == 0:\n",
    "            pad_1, pad_2 = pad_difference // 2, pad_difference // 2\n",
    "        else:\n",
    "            pad_1, pad_2 = pad_difference // 2, (pad_difference // 2) + 1\n",
    "\n",
    "        #Now applying the padding to the convolution layer\n",
    "        padding_layer = keras.layers.ZeroPadding1D(padding=(pad_1, pad_2))(con_1D)\n",
    "\n",
    "        #Then we merge\n",
    "        merged = keras.layers.Concatenate()([padding_layer,biLSTM])\n",
    "\n",
    "        #Then we reshape such that steps is the last item on that list\n",
    "        reshaped = Reshape((merged.shape[2], merged.shape[1])) (merged)\n",
    "\n",
    "\n",
    "        #Then we global maxpooling\n",
    "        max_pool_1D = GlobalMaxPooling1D()(reshaped)\n",
    "        print(f'max_pool_1D layer shape:', max_pool_1D.shape)\n",
    "\n",
    "        #Then the final output layer\n",
    "        output_layer = Dense(48)(max_pool_1D)\n",
    "\n",
    "        #Then pull everything together to build the final model\n",
    "        model = keras.models.Model(inputs=inputs, outputs=output_layer)\n",
    "        if printing == True:\n",
    "            print(model.summary())\n",
    "        if save_image:\n",
    "        # Save model architecture as a PNG image\n",
    "            plot_model(model, to_file=image_filename, show_shapes=True)\n",
    "        return model\n",
    "    \n",
    "    def model_training(self, model, X_train, y_train, X_val, y_val, lr_schedule):\n",
    "        '''Having built the model in the previous function, this method's focus is on training the model. It sets up the fixed hyperparameters and the callback functions to be used by the model when training'''\n",
    "\n",
    "        #SET UP TRAINING PARAMETERS\n",
    "\n",
    "        # Function to calculate root mean squared error (RMSE)\n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "        #epoch and batches\n",
    "        epochs = 5000\n",
    "        batch_size = int(X_train.shape[0]/8)\n",
    "        print(f'Batch size:', batch_size)\n",
    "\n",
    "        #CREATING MODEL CALLBACKS\n",
    "        #Implement the early stopping\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta= 1e-13, patience = 300)\n",
    "\n",
    "\n",
    "        #Compiling the model\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss = root_mean_squared_error, metrics = ['mse'])\n",
    "\n",
    "        #Fitting the model to the data\n",
    "        history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stop],\n",
    "        validation_data = (X_val, y_val)\n",
    "        )  \n",
    "        return history\n",
    "        \n",
    "\n",
    "    def optimize_hyperparameters(self, filepath, weeks = 2, date_1 = '2016-01-01 00:00:00', date_2 = '2017-01-01 00:00:00', date_3 = '2018-01-01 00:00:00', n_features = 1):\n",
    "\n",
    "        '''This method defines some of the parameters to be updated. Eventually, this should be moved to a separate dictionary that the user can easily input data to'''\n",
    "        pbounds = {\n",
    "            'con_1D_filters': (32, 128),\n",
    "            'con_1D_kernel_size': (2, 5),\n",
    "            'biLSTM_units': (50, 150),\n",
    "            'biLSTM_dropout_rate': (0.2, 0.5),\n",
    "            'learn_rate': (0.0005, 0.005),\n",
    "            'step1': (10, 500),\n",
    "            'step2': (1000, 2000),\n",
    "            'step3': (2000, 5000),\n",
    "            'rate1': (0.0001, 0.001),\n",
    "            'rate2': (0.00005, 0.0005),\n",
    "            'rate3': (0.000005, 0.00005)\n",
    "        }\n",
    "\n",
    "        # Prepare data\n",
    "        X_train, y_train, X_val, y_val,_,_ = self.prepare_data(filepath, weeks, date_1, date_2, date_3, n_features)\n",
    "        \n",
    "        #CREATING DATAFRAME TO SAVE RELEVANT RESULTS\n",
    "        results_df = pd.DataFrame(columns=[\"Iteration\", \"Start Time\", \"End Time\", \"con_1D_filters\", \"con_1D_kernel_size\", \"biLSTM_units\", \"biLSTM_dropout_rate\", 'biLSTM2_units',\"learn_rate\", \"step1\", \"step2\", \"step3\", \"rate1\", \"rate2\", \"rate3\", \"Train Loss\", \"Val Loss\", \"Train MSE\", \"Val MSE\"])\n",
    "\n",
    "\n",
    "        def evaluate_hyperparameters(con_1D_filters, con_1D_kernel_size, biLSTM_units, biLSTM_dropout_rate, learn_rate,step1,rate1,step2,rate2, step3, rate3):\n",
    "            '''This method focuses on trying out the different parameters for the changeable part of the model using the Bayesian Optimisation method.'''\n",
    "\n",
    "            #Set the adjusted learning rate for the different time periods. \n",
    "            initial_learning_rate = learn_rate\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay([int(step1), int(step2), int(step3)], [learn_rate, rate1,rate2,rate3])\n",
    "\n",
    "            #Here we define the start time\n",
    "            start_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "            # Build the model using the provided hyperparameters. Note that we are picking up from the model that we built before, except we ensured that the model hyperparameters were adjusted in the previous method to make them flexible enough for varying input\n",
    "            model = self.conv1D_bilstm_v2(int(con_1D_filters), int(con_1D_kernel_size), int(biLSTM_units), biLSTM_dropout_rate, printing = False, save_image=False)\n",
    "            \n",
    "            \n",
    "            # Train and evaluate the model using the prepared data\n",
    "            history = self.model_training(model, X_train, y_train, X_val, y_val, lr_schedule)\n",
    "\n",
    "\n",
    "            #Obtaining all the results and saving them to a dictionary\n",
    "            iteration_results = {\n",
    "                \"Iteration\": len(self.results_df) + 1,\n",
    "                \"Start Time\": start_time,\n",
    "                \"End Time\": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),\n",
    "                \"con_1D_filters\": int(con_1D_filters),\n",
    "                'con_1D_kernel_size': int(con_1D_kernel_size),\n",
    "                \"biLSTM_units\": int(biLSTM_units),\n",
    "                \"biLSTM_dropout_rate\": biLSTM_dropout_rate,\n",
    "                \"learn_rate\": learn_rate,\n",
    "                \"step1\": int(step1),\n",
    "                \"step2\": int(step2),\n",
    "                \"step3\": int(step3),\n",
    "                \"rate1\": rate1,\n",
    "                \"rate2\": rate2,\n",
    "                \"rate3\": rate3,\n",
    "                \"Train Loss\": history.history[\"loss\"][-1],\n",
    "                \"Val Loss\": history.history[\"val_loss\"][-1],\n",
    "                \"Train MSE\": history.history[\"mse\"][-1],\n",
    "                \"Val MSE\": history.history[\"val_mse\"][-1]\n",
    "            }\n",
    "\n",
    "            print(f'Results dataframe current length', iteration_results['Iteration'])\n",
    "\n",
    "            # Append iteration results to results_df\n",
    "            self.results_df = self.results_df.append(iteration_results, ignore_index=True)\n",
    "\n",
    "\n",
    "            # Save results to CSV after each iteration\n",
    "            self.results_df.to_csv('parallel_conv1D_biLSTM.csv', index=False)\n",
    "\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "            return -best_val_loss\n",
    "        \n",
    "        \n",
    "        optimizer = BayesianOptimization(\n",
    "            f=evaluate_hyperparameters,\n",
    "            pbounds=pbounds,\n",
    "            random_state=None,  # Set the random state as needed\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        # Run Bayesian Optimization\n",
    "        optimizer.maximize(init_points=15, n_iter=15)\n",
    "        \n",
    "        # Return the best hyperparameters found\n",
    "        best_hyperparameters = optimizer.max['params']\n",
    "        return best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING AN INSTANCE FOR NEW CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_instance = Con_1D_BiLSTM_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the prepare_data method to prepare the data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADING\n",
      "Input data runs from  2015-01-01 00:00:00 to 2019-12-31 23:30:00 and with 87648 datapoints\n",
      "Training duration is 672  timesteps, an equivalent of 2 weeks\n",
      "\n",
      "SPLITTING\n",
      "Split results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 28.726\n",
      "Val 18240 26.958\n",
      "Test 18192 28.007\n",
      "\n",
      "SCALING\n",
      "Scaled results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 0.31256682174126627\n",
      "Val 18240 0.26147889155373194\n",
      "Test 18192 0.34324056561289745\n",
      "\n",
      " RESHAPING\n",
      "Reshaping results\n",
      "_________________\n",
      "X_train (351, 672, 1)\n",
      "y_train (351, 48)\n",
      "X_val (366, 672)\n",
      "y_val (366, 48)\n",
      "X_test (365, 672, 1)\n",
      "y_test (365, 48)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = derived_instance.prepare_data('Demand_data', 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the conv1D_LSTM model builder with pre-fixed parameters to see if it actually builds the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_pool_1D layer shape: (None, 672)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 672, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 671, 32)              96        ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  (None, 672, 200)             81600     ['input_3[0][0]']             \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " zero_padding1d_2 (ZeroPadd  (None, 672, 32)              0         ['conv1d_2[0][0]']            \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 672, 200)             0         ['bidirectional_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 672, 232)             0         ['zero_padding1d_2[0][0]',    \n",
      " )                                                                   'dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 232, 672)             0         ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Gl  (None, 672)                  0         ['reshape_2[0][0]']           \n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 48)                   32304     ['global_max_pooling1d_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 114000 (445.31 KB)\n",
      "Trainable params: 114000 (445.31 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "derived_instance.conv1D_bilstm_v2(32, 2, 100, 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the optimiser method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADING\n",
      "Input data runs from  2015-01-01 00:00:00 to 2019-12-31 23:30:00 and with 87648 datapoints\n",
      "Training duration is 672  timesteps, an equivalent of 2 weeks\n",
      "\n",
      "SPLITTING\n",
      "Split results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 28.726\n",
      "Val 18240 26.958\n",
      "Test 18192 28.007\n",
      "\n",
      "SCALING\n",
      "Scaled results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 0.31256682174126627\n",
      "Val 18240 0.26147889155373194\n",
      "Test 18192 0.34324056561289745\n",
      "\n",
      " RESHAPING\n",
      "Reshaping results\n",
      "_________________\n",
      "X_train (351, 672, 1)\n",
      "y_train (351, 48)\n",
      "X_val (366, 672)\n",
      "y_val (366, 48)\n",
      "X_test (365, 672, 1)\n",
      "y_test (365, 48)\n",
      "|   iter    |  target   | biLSTM... | biLSTM... | con_1D... | con_1D... | learn_... |   rate1   |   rate2   |   rate3   |   step1   |   step2   |   step3   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_pool_1D layer shape: (None, 672)\n",
      "Batch size: 43\n",
      "Epoch 1/5000\n",
      "9/9 [==============================] - 27s 2s/step - loss: 0.2766 - mse: 0.0893 - val_loss: 0.1214 - val_mse: 0.0154\n",
      "Epoch 2/5000\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.1169 - mse: 0.0137 - val_loss: 0.1056 - val_mse: 0.0113\n",
      "Epoch 3/5000\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.1007 - mse: 0.0101 - val_loss: 0.0974 - val_mse: 0.0097\n",
      "Epoch 4/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0941 - mse: 0.0089 - val_loss: 0.0948 - val_mse: 0.0092\n",
      "Epoch 5/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0912 - mse: 0.0084 - val_loss: 0.0922 - val_mse: 0.0087\n",
      "Epoch 6/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0894 - mse: 0.0080 - val_loss: 0.0929 - val_mse: 0.0088\n",
      "Epoch 7/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0891 - mse: 0.0080 - val_loss: 0.0906 - val_mse: 0.0084\n",
      "Epoch 8/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0880 - mse: 0.0078 - val_loss: 0.0905 - val_mse: 0.0083\n",
      "Epoch 9/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0875 - mse: 0.0077 - val_loss: 0.0897 - val_mse: 0.0082\n",
      "Epoch 10/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0859 - mse: 0.0074 - val_loss: 0.0879 - val_mse: 0.0079\n",
      "Epoch 11/5000\n",
      "9/9 [==============================] - 17s 2s/step - loss: 0.0841 - mse: 0.0071 - val_loss: 0.0864 - val_mse: 0.0076\n",
      "Epoch 12/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0828 - mse: 0.0069 - val_loss: 0.0859 - val_mse: 0.0076\n",
      "Epoch 13/5000\n",
      "9/9 [==============================] - 20s 2s/step - loss: 0.0816 - mse: 0.0067 - val_loss: 0.0852 - val_mse: 0.0075\n",
      "Epoch 14/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0814 - mse: 0.0067 - val_loss: 0.0825 - val_mse: 0.0070\n",
      "Epoch 15/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0786 - mse: 0.0062 - val_loss: 0.0809 - val_mse: 0.0067\n",
      "Epoch 16/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0768 - mse: 0.0059 - val_loss: 0.0815 - val_mse: 0.0069\n",
      "Epoch 17/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0782 - mse: 0.0061 - val_loss: 0.0874 - val_mse: 0.0079\n",
      "Epoch 18/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0781 - mse: 0.0061 - val_loss: 0.0776 - val_mse: 0.0062\n",
      "Epoch 19/5000\n",
      "9/9 [==============================] - 21s 2s/step - loss: 0.0725 - mse: 0.0053 - val_loss: 0.0746 - val_mse: 0.0057\n",
      "Epoch 20/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0707 - mse: 0.0051 - val_loss: 0.0735 - val_mse: 0.0056\n",
      "Epoch 21/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0689 - mse: 0.0048 - val_loss: 0.0716 - val_mse: 0.0053\n",
      "Epoch 22/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0675 - mse: 0.0046 - val_loss: 0.0713 - val_mse: 0.0053\n",
      "Epoch 23/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0667 - mse: 0.0045 - val_loss: 0.0700 - val_mse: 0.0051\n",
      "Epoch 24/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0652 - mse: 0.0043 - val_loss: 0.0688 - val_mse: 0.0050\n",
      "Epoch 25/5000\n",
      "9/9 [==============================] - 19s 2s/step - loss: 0.0644 - mse: 0.0042 - val_loss: 0.0693 - val_mse: 0.0050\n",
      "Epoch 26/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0637 - mse: 0.0041 - val_loss: 0.0702 - val_mse: 0.0052\n",
      "Epoch 27/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0644 - mse: 0.0042 - val_loss: 0.0657 - val_mse: 0.0045\n",
      "Epoch 28/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0631 - mse: 0.0040 - val_loss: 0.0698 - val_mse: 0.0051\n",
      "Epoch 29/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0644 - mse: 0.0042 - val_loss: 0.0658 - val_mse: 0.0045\n",
      "Epoch 30/5000\n",
      "9/9 [==============================] - 18s 2s/step - loss: 0.0607 - mse: 0.0037 - val_loss: 0.0642 - val_mse: 0.0043\n",
      "Epoch 31/5000\n",
      "4/9 [============>.................] - ETA: 10s - loss: 0.0590 - mse: 0.0035"
     ]
    }
   ],
   "source": [
    "derived_instance.optimize_hyperparameters('Demand_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
