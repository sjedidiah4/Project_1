{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS THE PROGRAM FILE FOR ALL THE DIFFERENT MODELS UNDER TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from IPython.display import clear_output\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#We are inheriting all the properties available in that class (Data_prep) by passing it as an argument to the predictor class.\n",
    "from data_prep import Data_prep as prep\n",
    "\n",
    "class predictors(prep):\n",
    "    def __init__(self):\n",
    "        #Even though we inherited the properties, we had our parent class initialise certain attributes through the constructor (__init__). With inheritance, we need to explicitly invoke this initialisation through the use of super() method otherwise it it wil be bypassed\n",
    "        super().__init__()\n",
    "        self.results_df = pd.DataFrame(columns=[\"Iteration\", \"Start Time\", \"End Time\", \"con_1D_filters\", \"con_1D_kernel_size\", \"biLSTM_units\", \"biLSTM_dropout_rate\", 'biLSTM2_units',\"learn_rate\", \"step1\", \"step2\", \"step3\", \"rate1\", \"rate2\", \"rate3\", \"Train Loss\", \"Val Loss\", \"Train MSE\", \"Val MSE\"])\n",
    "\n",
    "    def prepare_data(self, filepath, weeks=2, date_1 = '2016-01-01 00:00:00', date_2 = '2017-01-01 00:00:00', date_3 = '2018-01-01 00:00:00', n_features = 1):\n",
    "\n",
    "        '''\n",
    "        Note that this is the method that inherits methods from the parent class and runs all the data preparation operations in one method call.\n",
    "\n",
    "        The method takes the path to the folder containing the input data, builds a dataframe, performs the splitting, scaling and eventual reshaping for the forecasting model. For more information, right click on the method in question and go to where it was declared.\n",
    "        \n",
    "        Inputs:\n",
    "        path_to_folder (mandatory and as a string)\n",
    "        \n",
    "        The number of weeks for training (as integer but optional. Default is 2 weeks)\n",
    "        \n",
    "        Date up to which the training split should occur (as a string. Example: '2016-01-01 00:00:00'. By default, set to '2016-01-01 00:00:00')\n",
    "         \n",
    "        Date up to which the validation split should occur (as string.Example: '2016-01-01 00:00:00'. By default set to '2017-01-01 00:00:00'). Note that once set, the split will happen from the date provided in step 3 to step 4 but with an overlapping window back into training set by the 2 weeks of training to ensure match in forecast dates for both validation and test sets\n",
    "        Date up to which the test split should occur (as string. Example: '2017-01-01 00:00:00'. By default, set to '2018-01-01 00:00:00'). To use the entire dataset, update this to the last date on your dataframe as printed when loaded.\n",
    "        \n",
    "        The number of features in your data. (as integer. Mandatory if dealing with multivariate models/ multi-feature datasets since by default, is set to 1 i.e single input feature.)\n",
    "        \n",
    "        Outputs:\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test\n",
    "        Note: Also prints on the terminal all the steps taken'''\n",
    "\n",
    "        #Loading csv - provide filepath and training weeks (optional)\n",
    "        training_duration = self.load_csv(filepath, weeks)\n",
    "        \n",
    "        #Splitting - training duration provided for by previous method.\n",
    "        # #Provide dates along which to perform the split (optional) - enter as string\n",
    "        train, val, test = self.train_val_test_split(training_duration, date_1, date_2, date_3)\n",
    "\n",
    "        #Scaling. All input data provided for by class methods\n",
    "        train_scaled, val_scaled, test_scaled = self.train_val_test_scaling(train, val, test)\n",
    "        \n",
    "        #Reshaping. Most input data provided for by previous class methods. Can update n_features depending on input features but by default, set to 1. \n",
    "        prepared_data = [train_scaled, training_duration, val_scaled, test_scaled]\n",
    "        \n",
    "        return self.reshaping(prepared_data, n_features)\n",
    "\n",
    "    def conv1D_bilstm(self, con_1D_filters, con_1D_kernel_size, biLSTM_units, biLSTM_dropout_rate, biLSTM2_units, printing = True):\n",
    "\n",
    "        '''This method's focus is to build the different neural layers and aggregate them into a single model. The method automatically prints out a model summary to help make sense of the model that has just been built\n",
    "\n",
    "\n",
    "        Input - the model parameters that the user wants to make adjustable for bayesian optimisation process. So far, these are set to: con_1D_filters, con_1D_kernel_size, biLSTM_units, biLSTM_dropout_rate, biLSTM2_units\n",
    "\n",
    "        \n",
    "        Output - model summary with the different layers, their output shapes and the overall learning parameters. While this is printed on the terminal for the user to see, the model itself is held by the method to be used in the optimisation process.'''\n",
    "\n",
    "        #Create the input node. We omit batch size when using functional API\n",
    "        inputs = keras.Input(shape = (X_train.shape[1], X_train.shape[2]))\n",
    "        \n",
    "\n",
    "        #Then we implement the convolution layer\n",
    "        con_1D = keras.layers.Conv1D(filters=con_1D_filters, kernel_size=con_1D_kernel_size)(inputs)\n",
    "\n",
    "        #Then we construct the parallel bilstm layer\n",
    "        biLSTM = Bidirectional(LSTM(biLSTM_units,return_sequences=True))(inputs)\n",
    "        biLSTM= Dropout(biLSTM_dropout_rate)(biLSTM)\n",
    "\n",
    "        # ADDING PADDING SEQUENCE TO MAKE THEM COMPATIBLE BEFORE CONCATENATION\n",
    "\n",
    "        #first we compute the length difference between the two outputs\n",
    "        pad_difference = (biLSTM.shape[1] - con_1D.shape[1])\n",
    "\n",
    "        #then we implement a for loop to ensure padding to equal lenth even if odd lenth difference\n",
    "        #// double division is to give us an integer rather than a float output\n",
    "        if (pad_difference) % 2 == 0:\n",
    "            pad_1, pad_2 = pad_difference // 2, pad_difference // 2\n",
    "        else:\n",
    "            pad_1, pad_2 = pad_difference // 2, (pad_difference // 2) + 1\n",
    "\n",
    "        #Now applying the padding to the convolution layer\n",
    "        padding_layer = keras.layers.ZeroPadding1D(padding=(pad_1, pad_2))(con_1D)\n",
    "\n",
    "        #Then we merge\n",
    "        merged = keras.layers.Concatenate()([padding_layer,biLSTM])\n",
    "\n",
    "        #Then we add a bilstm to condense the 264 variants into 1. Note, unlike in first bilstm, return-sequence set to false by not being declared\n",
    "        biLSTM_2 = Bidirectional(LSTM(biLSTM2_units))(merged)\n",
    "        biLSTM_2 = Dropout(0.5)(biLSTM_2)\n",
    "\n",
    "        #Then the final output layer\n",
    "        output_layer = Dense(48)(biLSTM_2)\n",
    "\n",
    "        #Then pull everything together to build the final model\n",
    "        model = keras.models.Model(inputs=inputs, outputs=output_layer)\n",
    "        if printing == True:\n",
    "            print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def model_training(self, model, X_train, y_train, X_val, y_val, lr_schedule):\n",
    "        '''Having built the model in the previous function, this method's focus is on training the model. It sets up the fixed hyperparameters and the callback functions to be used by the model when training'''\n",
    "\n",
    "        #SET UP TRAINING PARAMETERS\n",
    "\n",
    "        # Function to calculate root mean squared error (RMSE)\n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "        #epoch and batches\n",
    "        epochs = 5000\n",
    "        batch_size = int(X_train.shape[0]/8)\n",
    "        print(f'Batch size:', batch_size)\n",
    "\n",
    "        #CREATING MODEL CALLBACKS\n",
    "        #Implement the early stopping\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', min_delta= 1e-13, patience = 300)\n",
    "\n",
    "\n",
    "        #Compiling the model\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss = root_mean_squared_error, metrics = ['mse'])\n",
    "\n",
    "        #Fitting the model to the data\n",
    "        history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stop],\n",
    "        validation_data = (X_val, y_val)\n",
    "        )  \n",
    "        return history\n",
    "        \n",
    "\n",
    "    def optimize_hyperparameters(self, filepath, weeks = 2, date_1 = '2016-01-01 00:00:00', date_2 = '2017-01-01 00:00:00', date_3 = '2018-01-01 00:00:00', n_features = 1):\n",
    "\n",
    "        '''This method defines some of the parameters to be updated. Eventually, this should be moved to a separate dictionary that the user can easily input data to'''\n",
    "        pbounds = {\n",
    "            'con_1D_filters': (32, 128),\n",
    "            'con_1D_kernel_size': (2, 5),\n",
    "            'biLSTM_units': (50, 150),\n",
    "            'biLSTM_dropout_rate': (0.2, 0.5),\n",
    "            'biLSTM2_units': (50, 150),\n",
    "            'learn_rate': (0.0005, 0.005),\n",
    "            'step1': (10, 500),\n",
    "            'step2': (1000, 2000),\n",
    "            'step3': (2000, 5000),\n",
    "            'rate1': (0.0001, 0.001),\n",
    "            'rate2': (0.00005, 0.0005),\n",
    "            'rate3': (0.000005, 0.00005)\n",
    "        }\n",
    "\n",
    "        # Prepare data\n",
    "        X_train, y_train, X_val, y_val,_,_ = self.prepare_data(filepath, weeks, date_1, date_2, date_3, n_features)\n",
    "        \n",
    "        #CREATING DATAFRAME TO SAVE RELEVANT RESULTS\n",
    "        results_df = pd.DataFrame(columns=[\"Iteration\", \"Start Time\", \"End Time\", \"con_1D_filters\", \"con_1D_kernel_size\", \"biLSTM_units\", \"biLSTM_dropout_rate\", 'biLSTM2_units',\"learn_rate\", \"step1\", \"step2\", \"step3\", \"rate1\", \"rate2\", \"rate3\", \"Train Loss\", \"Val Loss\", \"Train MSE\", \"Val MSE\"])\n",
    "\n",
    "\n",
    "        def evaluate_hyperparameters(con_1D_filters, con_1D_kernel_size, biLSTM_units, biLSTM_dropout_rate, biLSTM2_units, learn_rate,step1,rate1,step2,rate2, step3, rate3):\n",
    "            '''This method focuses on trying out the different parameters for the changeable part of the model using the Bayesian Optimisation method.'''\n",
    "\n",
    "            #Set the adjusted learning rate for the different time periods. \n",
    "            initial_learning_rate = learn_rate\n",
    "            lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay([int(step1), int(step2), int(step3)], [learn_rate, rate1,rate2,rate3])\n",
    "\n",
    "            #Here we define the start time\n",
    "            start_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "            # Build the model using the provided hyperparameters. Note that we are picking up from the model that we built before, except we ensured that the model hyperparameters were adjusted in the previous method to make them flexible enough for varying input\n",
    "            model = self.conv1D_bilstm(int(con_1D_filters), int(con_1D_kernel_size), int(biLSTM_units), biLSTM_dropout_rate, int(biLSTM2_units), printing = False)\n",
    "            \n",
    "            \n",
    "            # Train and evaluate the model using the prepared data\n",
    "            history = self.model_training(model, X_train, y_train, X_val, y_val, lr_schedule)\n",
    "\n",
    "\n",
    "            #Obtaining all the results and saving them to a dictionary\n",
    "            iteration_results = {\n",
    "                \"Iteration\": len(self.results_df) + 1,\n",
    "                \"Start Time\": start_time,\n",
    "                \"End Time\": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),\n",
    "                \"con_1D_filters\": int(con_1D_filters),\n",
    "                'con_1D_kernel_size': int(con_1D_kernel_size),\n",
    "                \"biLSTM_units\": int(biLSTM_units),\n",
    "                \"biLSTM_dropout_rate\": biLSTM_dropout_rate,\n",
    "                \"biLSTM2_units\": int(biLSTM2_units),\n",
    "                \"learn_rate\": learn_rate,\n",
    "                \"step1\": int(step1),\n",
    "                \"step2\": int(step2),\n",
    "                \"step3\": int(step3),\n",
    "                \"rate1\": rate1,\n",
    "                \"rate2\": rate2,\n",
    "                \"rate3\": rate3,\n",
    "                \"Train Loss\": history.history[\"loss\"][-1],\n",
    "                \"Val Loss\": history.history[\"val_loss\"][-1],\n",
    "                \"Train MSE\": history.history[\"mse\"][-1],\n",
    "                \"Val MSE\": history.history[\"val_mse\"][-1]\n",
    "            }\n",
    "\n",
    "            print(f'Results dataframe current length', iteration_results['Iteration'])\n",
    "\n",
    "            # Append iteration results to results_df\n",
    "            self.results_df = self.results_df.append(iteration_results, ignore_index=True)\n",
    "\n",
    "\n",
    "            # Save results to CSV after each iteration\n",
    "            self.results_df.to_csv('parallel_conv1D_biLSTM.csv', index=False)\n",
    "\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "            return -best_val_loss\n",
    "        \n",
    "        \n",
    "        optimizer = BayesianOptimization(\n",
    "            f=evaluate_hyperparameters,\n",
    "            pbounds=pbounds,\n",
    "            random_state=None,  # Set the random state as needed\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        # Run Bayesian Optimization\n",
    "        optimizer.maximize(init_points=15, n_iter=15)\n",
    "        \n",
    "        # Return the best hyperparameters found\n",
    "        best_hyperparameters = optimizer.max['params']\n",
    "        return best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CREATE AN OBJECT/INSTANCE FOR THE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_instance = predictors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the prepare_data method to prepare the data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADING\n",
      "Input data runs from  2015-01-01 00:00:00 to 2019-12-31 23:30:00 and with 87648 datapoints\n",
      "Training duration is 672  timesteps, an equivalent of 2 weeks\n",
      "\n",
      "SPLITTING\n",
      "Split results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 28.726\n",
      "Val 18240 26.958\n",
      "Test 18192 28.007\n",
      "\n",
      "SCALING\n",
      "Scaled results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 0.31256682174126627\n",
      "Val 18240 0.26147889155373194\n",
      "Test 18192 0.34324056561289745\n",
      "\n",
      " RESHAPING\n",
      "Reshaping results\n",
      "_________________\n",
      "X_train (351, 672, 1)\n",
      "y_train (351, 48)\n",
      "X_val (366, 672)\n",
      "y_val (366, 48)\n",
      "X_test (365, 672, 1)\n",
      "y_test (365, 48)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = derived_instance.prepare_data('Demand_data', 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the conv1D_LSTM model builder with pre-fixed parameters to see if it actually builds the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 672, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 671, 32)              96        ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 672, 200)             81600     ['input_1[0][0]']             \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " zero_padding1d (ZeroPaddin  (None, 672, 32)              0         ['conv1d[0][0]']              \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 672, 200)             0         ['bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 672, 232)             0         ['zero_padding1d[0][0]',      \n",
      "                                                                     'dropout[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 200)                  266400    ['concatenate[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 200)                  0         ['bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 48)                   9648      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357744 (1.36 MB)\n",
      "Trainable params: 357744 (1.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x20e410763b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_instance.conv1D_bilstm(32, 2, 100, 0.5, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the optimiser method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADING\n",
      "Input data runs from  2015-01-01 00:00:00 to 2019-12-31 23:30:00 and with 87648 datapoints\n",
      "Training duration is 672  timesteps, an equivalent of 2 weeks\n",
      "\n",
      "SPLITTING\n",
      "Split results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 28.726\n",
      "Val 18240 26.958\n",
      "Test 18192 28.007\n",
      "\n",
      "SCALING\n",
      "Scaled results\n",
      "_____________\n",
      "Set Length Datapoint_1\n",
      "Train 17520 0.31256682174126627\n",
      "Val 18240 0.26147889155373194\n",
      "Test 18192 0.34324056561289745\n",
      "\n",
      " RESHAPING\n",
      "Reshaping results\n",
      "_________________\n",
      "X_train (351, 672, 1)\n",
      "y_train (351, 48)\n",
      "X_val (366, 672)\n",
      "y_val (366, 48)\n",
      "X_test (365, 672, 1)\n",
      "y_test (365, 48)\n",
      "|   iter    |  target   | biLSTM... | biLSTM... | biLSTM... | con_1D... | con_1D... | learn_... |   rate1   |   rate2   |   rate3   |   step1   |   step2   |   step3   |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Batch size: 43\n",
      "Epoch 1/5000\n",
      "6/9 [===================>..........] - ETA: 15s - loss: 0.3467 - mse: 0.1260"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m derived_instance\u001b[39m.\u001b[39;49moptimize_hyperparameters(\u001b[39m'\u001b[39;49m\u001b[39mDemand_data\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[1], line 238\u001b[0m, in \u001b[0;36mpredictors.optimize_hyperparameters\u001b[1;34m(self, filepath, weeks, date_1, date_2, date_3, n_features)\u001b[0m\n\u001b[0;32m    230\u001b[0m optimizer \u001b[39m=\u001b[39m BayesianOptimization(\n\u001b[0;32m    231\u001b[0m     f\u001b[39m=\u001b[39mevaluate_hyperparameters,\n\u001b[0;32m    232\u001b[0m     pbounds\u001b[39m=\u001b[39mpbounds,\n\u001b[0;32m    233\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# Set the random state as needed\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m    235\u001b[0m )\n\u001b[0;32m    237\u001b[0m \u001b[39m# Run Bayesian Optimization\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m optimizer\u001b[39m.\u001b[39;49mmaximize(init_points\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n\u001b[0;32m    240\u001b[0m \u001b[39m# Return the best hyperparameters found\u001b[39;00m\n\u001b[0;32m    241\u001b[0m best_hyperparameters \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mmax[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bayes_opt\\bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuggest(util)\n\u001b[0;32m    309\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 310\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobe(x_probe, lazy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer \u001b[39mand\u001b[39;00m iteration \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    313\u001b[0m     \u001b[39m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_bounds(\n\u001b[0;32m    316\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bayes_opt\\bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue\u001b[39m.\u001b[39madd(params)\n\u001b[0;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_space\u001b[39m.\u001b[39;49mprobe(params)\n\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch(Events\u001b[39m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bayes_opt\\target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    234\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_as_array(params)\n\u001b[0;32m    235\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, x))\n\u001b[1;32m--> 236\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constraint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[1;32mIn[1], line 190\u001b[0m, in \u001b[0;36mpredictors.optimize_hyperparameters.<locals>.evaluate_hyperparameters\u001b[1;34m(con_1D_filters, con_1D_kernel_size, biLSTM_units, biLSTM_dropout_rate, biLSTM2_units, learn_rate, step1, rate1, step2, rate2, step3, rate3)\u001b[0m\n\u001b[0;32m    186\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1D_bilstm(\u001b[39mint\u001b[39m(con_1D_filters), \u001b[39mint\u001b[39m(con_1D_kernel_size), \u001b[39mint\u001b[39m(biLSTM_units), biLSTM_dropout_rate, \u001b[39mint\u001b[39m(biLSTM2_units), printing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39m# Train and evaluate the model using the prepared data\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_training(model, X_train, y_train, X_val, y_val, lr_schedule)\n\u001b[0;32m    193\u001b[0m \u001b[39m#Obtaining all the results and saving them to a dictionary\u001b[39;00m\n\u001b[0;32m    194\u001b[0m iteration_results \u001b[39m=\u001b[39m {\n\u001b[0;32m    195\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIteration\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults_df) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    196\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStart Time\u001b[39m\u001b[39m\"\u001b[39m: start_time,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mVal MSE\u001b[39m\u001b[39m\"\u001b[39m: history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_mse\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    214\u001b[0m }\n",
      "Cell \u001b[1;32mIn[1], line 139\u001b[0m, in \u001b[0;36mpredictors.model_training\u001b[1;34m(self, model, X_train, y_train, X_val, y_val, lr_schedule)\u001b[0m\n\u001b[0;32m    136\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlr_schedule), loss \u001b[39m=\u001b[39m root_mean_squared_error, metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    138\u001b[0m \u001b[39m#Fitting the model to the data\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    140\u001b[0m X_train,\n\u001b[0;32m    141\u001b[0m y_train,\n\u001b[0;32m    142\u001b[0m batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    143\u001b[0m epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m    144\u001b[0m callbacks\u001b[39m=\u001b[39;49m[early_stop],\n\u001b[0;32m    145\u001b[0m validation_data \u001b[39m=\u001b[39;49m (X_val, y_val)\n\u001b[0;32m    146\u001b[0m )  \n\u001b[0;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "derived_instance.optimize_hyperparameters('Demand_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the model that we just built to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = derived_instance.conv1D_bilstm();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the model training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_instance.model_training(model, X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
